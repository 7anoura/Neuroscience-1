# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BKBLHi7tNevs_9hCmhns8gAQiKfh96m0
"""

import numpy as np

words = ["I", "love", "neural", "networks"]
vocab_size = len(words)

word_to_idx = {word: i for i, word in enumerate(words)}
idx_to_word = {i: word for i, word in enumerate(words)}

def one_hot_encode(word, vocab_size):
    vec = np.zeros(vocab_size)
    vec[word_to_idx[word]] = 1
    return vec

input_sequence = ["I", "love", "neural"]
target_word = "networks"

X = [one_hot_encode(word, vocab_size) for word in input_sequence]
y = one_hot_encode(target_word, vocab_size)

hidden_size = 5
learning_rate = 0.1

Wxh = np.random.randn(hidden_size, vocab_size) * 0.01
Whh = np.random.randn(hidden_size, hidden_size) * 0.01
Why = np.random.randn(vocab_size, hidden_size) * 0.01
bh = np.zeros((hidden_size, 1))
by = np.zeros((vocab_size, 1))

def forward_pass(X):
    h_prev = np.zeros((hidden_size, 1))

    hidden_states = {}
    outputs = {}

    for t in range(len(X)):
        x = np.reshape(X[t], (vocab_size, 1))

        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)

        y = np.dot(Why, h) + by

        hidden_states[t] = h
        outputs[t] = y

        h_prev = h

    return hidden_states, outputs

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

def calculate_loss(outputs, y):
    last_output = outputs[len(outputs)-1]

    probs = softmax(last_output)

    loss = -np.log(probs[np.argmax(y)])
    return loss, probs

def backprop(X, hidden_states, dy):
    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
    dbh, dby = np.zeros_like(bh), np.zeros_like(by)
    dh_next = np.zeros_like(bh)

    for t in reversed(range(len(X))):
        x = np.reshape(X[t], (vocab_size, 1))
        h = hidden_states[t]
        h_prev = hidden_states[t-1] if t > 0 else np.zeros_like(h)

        dWhy += np.dot(dy, h.T)
        dby += dy

        dh = np.dot(Why.T, dy) + dh_next
        dh_raw = (1 - h * h) * dh

        dbh += dh_raw
        dWxh += np.dot(dh_raw, x.T)
        dWhh += np.dot(dh_raw, h_prev.T)
        dh_next = np.dot(Whh.T, dh_raw)

    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam)

    return dWxh, dWhh, dWhy, dbh, dby

def train(X, y, num_epochs=100):
    global Wxh, Whh, Why, bh, by

    for epoch in range(num_epochs):
        hidden_states, outputs = forward_pass(X)
        loss, probs = calculate_loss(outputs, y)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {loss[0]:.4f}")
            predicted_word = idx_to_word[np.argmax(probs)]
            print(f"Predicted word: {predicted_word}")

        dy = probs - y.reshape(-1, 1)
        dWxh, dWhh, dWhy, dbh, dby = backprop(X, hidden_states, dy)

        Wxh -= learning_rate * dWxh
        Whh -= learning_rate * dWhh
        Why -= learning_rate * dWhy
        bh -= learning_rate * dbh
        by -= learning_rate * dby

train(X, y, num_epochs=100)

def predict(X):
    hidden_states, outputs = forward_pass(X)
    last_output = outputs[len(outputs)-1]
    probs = softmax(last_output)
    predicted_index = np.argmax(probs)
    return idx_to_word[predicted_index]

test_sequence = ["I", "love", "neural"]
test_X = [one_hot_encode(word, vocab_size) for word in test_sequence]
predicted_word = predict(test_X)
print(f"\nFinal prediction: {predicted_word}")

